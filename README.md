# BigQuery Weather Report Data Warehouse
O projeto se trata de uma pipeline ETL automatizada que constr√≥i e alimenta um data lake(camadas Bronze e Silver) e um data warehouse(camada Gold) com dados vindos das APIs do IBGE(dados sobre cidades) e da CPTEC(dados de previs√£o do tempo).

O projeto mistura elementos on-premises(executados localmente) como o a pipeline automatizada com Airflow e o bucket MinIO(data lake) com elementos em nuvem como o BigQuery e a Dashboard.

Diagrama do projeto:
![Diagrama do Projeto](.github/images/project_diagram.png)

## Setup

### 1. Com Airflow e Docker:

#### 1.1 Baixar os arquivos deste reposit√≥rio.
O primeiro passo para fazer o projeto funcionar √© baixar os arquivos deste reposit√≥rio para a sua m√°quina, isso pode ser feito navegando por meio de um terminal at√© o diret√≥rio onde voc√™ deseja colocar os arquivos e executar o comando `git clone https://github.com/VictorClvtt/weather-bigquery-warehouse.git` ou simplesmente baixando os arquivos pela interface do GitHub e extraindo o .zip onde desejar.

#### 1.2 Criar um projeto Astro.
Para isso ser√° necess√°rio j√° ter instalado na sua m√°quina os softwares `astro-cli`, `docker` e `docker-compose`.
Supondo que essas dependencias j√° est√£o instaladas basta criar um diret√≥rio para o seu projeto Airflow e executar o comando `astro-cli dev init` para iniciar um projeto Airflow no diret√≥rio desejado.

Depois disso voc√™ deve mover os arquivos do diret√≥rio [airflow/](airflow/) deste reposit√≥rio para o diret√≥rio do novo projeto Airflow e sobrescrever arquivos/diret√≥rios que tenham nomes iguais a de arquivos/diret√≥rios que est√£o sendo movido para o diret√≥rio do projeto Airflow.

Tamb√©m √© importante que dentro do diret√≥rio [airflow/include/](airflow/include/) sendo copiado voc√™ crie um arquivo `.env` com a vari√°vel "GOOGLE_APPLICATION_CREDENTIALS" sendo o valor dela o caminho para um arquivo .json com suas credenciais da API do BigQuery que voc√™ deve criar atrav√©s do console do GCP e armazenar idealmente no mesmo diret√≥rio do `.env`.

#### 1.3 Executar o projeto Astro.
Com todos os arquivos necess√°rios em seus devidos lugares, basta executar o comando `astro-cli dev start` para que os containers Docker com os componentes do projeto(Airflow, MinIO, Spark Master e Spark Worker) sejam inicializados.

Ap√≥s a inicializa√ß√£o terminar a pipeline ja vai estar funcionando, orquestrada pelo Airflow.

Para acessar a interface web do MinIO basta acessar https://127.0.0.1:9000 e inserir o nome de usuario padr√£o "admin" e a senha padr√£o "admin123" para logar. Para a interface web do Airflow basta acessar https://127.0.0.1:8080 e inserir o nome de usuario padr√£o "admin" e a senha padr√£o "admin" para logar.

### 2. Com Virtual Env local:

#### 2.1 Baixar os arquivos deste reposit√≥rio.
O primeiro passo para fazer o projeto funcionar √© baixar os arquivos deste reposit√≥rio para a sua m√°quina, isso pode ser feito navegando por meio de um terminal at√© o diret√≥rio onde voc√™ deseja colocar os arquivos e executar o comando `git clone https://github.com/VictorClvtt/weather-bigquery-warehouse.git` ou simplesmente baixando os arquivos pela interface do GitHub e extraindo o .zip onde desejar.

#### 2.2 Preparar credenciais.
√â importante que dentro do diret√≥rio raiz do projeto voc√™ crie um arquivo `.env` com a vari√°vel "GOOGLE_APPLICATION_CREDENTIALS" sendo o valor dela o caminho para um arquivo .json com suas credenciais da API do BigQuery que voc√™ deve criar atrav√©s do console do GCP e armazenar idealmente no mesmo diret√≥rio do `.env`.

#### 2.3 Subir um container MinIO.
J√° que este m√©todo de setup serve apenas para execu√ß√µes manuais e testes, ser√° necess√°rio subir apena um container para o MinIO com o seguinte comando:
```bash
docker run -d \
  --name minio \
  -p 9000:9000 \
  -p 9001:9001 \
  -e MINIO_ROOT_USER=admin \
  -e MINIO_ROOT_PASSWORD=admin123 \
  quay.io/minio/minio server /data --console-address ":9001"
```
Ap√≥s executar esse comando o MinIO estar√° rodando, √© poss√≠vel acessar a interface web do MinIO acessando https://127.0.0.1:9000 e inserindo o nome de usuario padr√£o "admin" e a senha padr√£o "admin123" para logar.

#### 2.3 Criar o Virtual Env.
Para criar o virtual env basta executar o comando `python -m venv .venv` para criar o ambiente, depois executar `source .venv/bin/activate` para ativar o ambiente e por fim executar `pip install -r requirements.txt` para baixar todas as dependencias necess√°rias para executar o projeto dessa forma.

Com o .venv criado j√° √© poss√≠vel executar o projeto manualmente, basta utilizar algum atalho da sua IDE para executar os scripts ou executa-los atrav√©s do terminal utilizando o ambiente criado.

## Etapas

### 1. [bronze_ingest.py](src/etl/bronze_ingest.py): Script de ingest√£o de dados.
O primeiro script √© respons√°vel por fazer requisi√ß√µes √†s APIs do IBGE e da CPTEC, a API do IBGE retorna dados basicos sobre todas as cidades do estado de S√£o Paulo, enquanto a API da CPTEC retorna dados de previs√£o do tempo de at√© seis dias ap√≥s a data da requisi√ß√£o a partir dos nomes das cidades retornados pela API do IBGE. Com os dados em mem√≥ria o script faz uma pequena transforma√ß√£o que torna os dados tabulares para que sejam gravados em formato CSV na camada Bronze do data lake.

O script faz isso utilizando principalmente as bibliotecas requests para fazer as requisi√ß√µes, a biblioteca Pandas para mudar a estrutura dos dados e a biblioteca Boto3 para gravar os dados em formato CSV no data lake.

### 2. [bronze_to_silver.py](src/etl/bronze_to_silver.py): Script de limpeza e formata√ß√£o de dados.
O segundo script √© respons√°vel por padronizar, procurar por inconsist√™ncias e defeitos nos dados vindos da camada Bronze e gravar os dados limpos em formato Parquet na camada Silver do data lake.

O script faz isso utilizando principalmente o framework PySpark que possibilita a leitura dos dados para um tipo de dados(DataFrame) que oferece diversos m√©todos e atributos que permitem explorar e processar os dados com facilidade e rapidez.

### 3. [silver_to_gold.py](src/etl/silver_to_gold.py): Script de modelagem de dados.
O terceiro script √© respons√°vel por criar um modelo dimensional de banco de dados com os dados vindos da camada Silver e gravar os dados na camada Gold do data warehouse no BigQuery.

ü•à Schemas das tabelas da camada Silver:
```
üìÉ "silver_ibge_cities" table schema:
root
 |-- id: integer (nullable = true)
 |-- nome: string (nullable = true)
 |-- microrregiao_id: integer (nullable = true)
 |-- microrregiao_nome: string (nullable = true)
 |-- microrregiao_mesorregiao_id: integer (nullable = true)
 |-- microrregiao_mesorregiao_nome: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_id: integer (nullable = true)
 |-- microrregiao_mesorregiao_UF_sigla: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_nome: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_regiao_id: integer (nullable = true)
 |-- microrregiao_mesorregiao_UF_regiao_sigla: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_regiao_nome: string (nullable = true)
 |-- regiao_imediata_id: integer (nullable = true)
 |-- regiao_imediata_nome: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_id: integer (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_nome: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_id: integer (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_sigla: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_nome: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_regiao_id: integer (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_regiao_sigla: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_regiao_nome: string (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)

üìÉ "silver_cptec_cities" table schema:
root
 |-- nome: string (nullable = true)
 |-- id: integer (nullable = true)
 |-- estado: string (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)

üìÉ "silver_cptec_weather" table schema:
root
 |-- cidade: string (nullable = true)
 |-- estado: string (nullable = true)
 |-- atualizado_em: date (nullable = true)
 |-- data: date (nullable = true)
 |-- condicao: string (nullable = true)
 |-- condicao_desc: string (nullable = true)
 |-- min: integer (nullable = true)
 |-- max: integer (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)
```

ü•á Schemas das tabelas remodeladas da camada Gold:
```
üìÉ "dim_city" table schema:
root
 |-- id_ibge: integer (nullable = true)
 |-- id_cptec: integer (nullable = true)
 |-- nome: string (nullable = true)
 |-- nome_regiao: string (nullable = true)
 |-- uf_sigla: string (nullable = true)
 |-- uf_nome: string (nullable = true)
 |-- id_city: string (nullable = true)

üìÉ "dim_update_date" table schema:
root
 |-- data: date (nullable = true)
 |-- id_update_date: string (nullable = true)

üìÉ "dim_forecast_date" table schema:
root
 |-- data: date (nullable = true)
 |-- id_forecast_date: string (nullable = true)

üìÉ "dim_weather_condition" table schema:
root
 |-- condicao: string (nullable = true)
 |-- condicao_desc: string (nullable = true)
 |-- id_weather_condition: string (nullable = true)

üìÉ "fact_weather" table schema:
root
 |-- id_city: string (nullable = true)
 |-- id_update_date: string (nullable = true)
 |-- id_forecast_date: string (nullable = true)
 |-- id_weather_condition: string (nullable = true)
 |-- temperatura_min: integer (nullable = true)
 |-- temperatura_max: integer (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)
 |-- _modeling_date: string (nullable = false)
 |-- id_fact: string (nullable = true)
```

O script faz isso tamb√©m utilizando principalmente o framework PySpark para ler os dados e criar um modelo dimensional Star Schema com os dados da camada Silver para permitir que esses dados sejam consumidos de forma mais f√°cil e eficiente para fins anal√≠ticos.

### 4. [Airflow](airflow/): Automa√ß√£o e agendamento da pipeline.
Nessa etapa do projeto foi criada uma DAG com Airflow que automatiza e define as depend√™ncias entre as Tasks da pipeline.

O setup de todo o ambiente, tanto pro Airflow quanto pro Spark e pro MinIO foi feito utilizando Docker, o que permite a reproducibilidade do ambiente e minimiza erros de execu√ß√£o da pipeline.

### 5. Dashboard: Painel interativo utilizando Looker Studio
Com o uso do servi√ßo de cria√ß√£o de pain√©is interativos Looker Studio tamb√©m do Google, foi poss√≠vel com facilidade criar um Dashboard interativo com gr√°ficos alimentados pelos dados clim√°ticos de cada cidade.