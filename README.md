# BigQuery Weather Report Data Warehouse
O projeto se trata de uma pipeline ETL automatizada que constrÃ³i e alimenta um data lake(camadas Bronze e Silver) e um data warehouse(camada Gold) com dados vindos das APIs do IBGE(dados sobre cidades) e da CPTEC(dados de previsÃ£o do tempo).

O projeto mistura elementos on-premises(executados localmente) como o a pipeline automatizada com Airflow e o bucket MinIO(data lake) com elementos em nuvem como o BigQuery e a Dashboard.

Diagrama do projeto:
![Diagrama do Projeto](.github/images/project_diagram.png)


## Etapas

### 1. [bronze_ingest.py](src/etl/bronze_ingest.py): Script de ingestÃ£o de dados.
O primeiro script Ã© responsÃ¡vel por fazer requisiÃ§Ãµes Ã s APIs do IBGE e da CPTEC, a API do IBGE retorna dados basicos sobre todas as cidades do estado de SÃ£o Paulo, enquanto a API da CPTEC retorna dados de previsÃ£o do tempo de atÃ© seis dias apÃ³s a data da requisiÃ§Ã£o a partir dos nomes das cidades retornados pela API do IBGE. Com os dados em memÃ³ria o script faz uma pequena transformaÃ§Ã£o que torna os dados tabulares para que sejam gravados em formato CSV na camada Bronze do data lake.

O script faz isso utilizando principalmente as bibliotecas requests para fazer as requisiÃ§Ãµes, a biblioteca Pandas para mudar a estrutura dos dados e a biblioteca Boto3 para gravar os dados em formato CSV no data lake.

### 2. [bronze_to_silver.py](src/etl/bronze_to_silver.py): Script de limpeza e formataÃ§Ã£o de dados.
O segundo script Ã© responsÃ¡vel por padronizar, procurar por inconsistÃªncias e defeitos nos dados vindos da camada Bronze e gravar os dados limpos em formato Parquet na camada Silver do data lake.

O script faz isso utilizando principalmente o framework PySpark que possibilita a leitura dos dados para um tipo de dados(DataFrame) que oferece diversos mÃ©todos e atributos que permitem explorar e processar os dados com facilidade e rapidez.

### 3. [silver_to_gold.py](src/etl/silver_to_gold.py): Script de modelagem de dados.
O terceiro script Ã© responsÃ¡vel por criar um modelo dimensional de banco de dados com os dados vindos da camada Silver e gravar os dados na camada Gold do data warehouse no BigQuery.

ðŸ¥ˆ Schemas das tabelas da camada Silver:
```
ðŸ“ƒ "silver_ibge_cities" table schema:
root
 |-- id: integer (nullable = true)
 |-- nome: string (nullable = true)
 |-- microrregiao_id: integer (nullable = true)
 |-- microrregiao_nome: string (nullable = true)
 |-- microrregiao_mesorregiao_id: integer (nullable = true)
 |-- microrregiao_mesorregiao_nome: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_id: integer (nullable = true)
 |-- microrregiao_mesorregiao_UF_sigla: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_nome: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_regiao_id: integer (nullable = true)
 |-- microrregiao_mesorregiao_UF_regiao_sigla: string (nullable = true)
 |-- microrregiao_mesorregiao_UF_regiao_nome: string (nullable = true)
 |-- regiao_imediata_id: integer (nullable = true)
 |-- regiao_imediata_nome: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_id: integer (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_nome: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_id: integer (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_sigla: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_nome: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_regiao_id: integer (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_regiao_sigla: string (nullable = true)
 |-- regiao_imediata_regiao_intermediaria_UF_regiao_nome: string (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)

ðŸ“ƒ "silver_cptec_cities" table schema:
root
 |-- nome: string (nullable = true)
 |-- id: integer (nullable = true)
 |-- estado: string (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)

ðŸ“ƒ "silver_cptec_weather" table schema:
root
 |-- cidade: string (nullable = true)
 |-- estado: string (nullable = true)
 |-- atualizado_em: date (nullable = true)
 |-- data: date (nullable = true)
 |-- condicao: string (nullable = true)
 |-- condicao_desc: string (nullable = true)
 |-- min: integer (nullable = true)
 |-- max: integer (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)
```

ðŸ¥‡ Schemas das tabelas remodeladas da camada Gold:
```
ðŸ“ƒ "dim_city" table schema:
root
 |-- id_ibge: integer (nullable = true)
 |-- id_cptec: integer (nullable = true)
 |-- nome: string (nullable = true)
 |-- nome_regiao: string (nullable = true)
 |-- uf_sigla: string (nullable = true)
 |-- uf_nome: string (nullable = true)
 |-- id_city: string (nullable = true)

ðŸ“ƒ "dim_update_date" table schema:
root
 |-- data: date (nullable = true)
 |-- id_update_date: string (nullable = true)

ðŸ“ƒ "dim_forecast_date" table schema:
root
 |-- data: date (nullable = true)
 |-- id_forecast_date: string (nullable = true)

ðŸ“ƒ "dim_weather_condition" table schema:
root
 |-- condicao: string (nullable = true)
 |-- condicao_desc: string (nullable = true)
 |-- id_weather_condition: string (nullable = true)

ðŸ“ƒ "fact_weather" table schema:
root
 |-- id_city: string (nullable = true)
 |-- id_update_date: string (nullable = true)
 |-- id_forecast_date: string (nullable = true)
 |-- id_weather_condition: string (nullable = true)
 |-- temperatura_min: integer (nullable = true)
 |-- temperatura_max: integer (nullable = true)
 |-- _source: string (nullable = true)
 |-- _ingestion_date: date (nullable = true)
 |-- _processing_date: date (nullable = true)
 |-- _modeling_date: string (nullable = false)
 |-- id_fact: string (nullable = true)
```

O script faz isso tambÃ©m utilizando principalmente o framework PySpark para ler os dados e criar um modelo dimensional Star Schema com os dados da camada Silver para permitir que esses dados sejam consumidos de forma mais fÃ¡cil e eficiente para fins analÃ­ticos.

### 4. [Airflow](airflow/): AutomaÃ§Ã£o e agendamento da pipeline.
Nessa etapa do projeto foi criada uma DAG com Airflow que automatiza e define as dependÃªncias entre as Tasks da pipeline.

O setup de todo o ambiente, tanto pro Airflow quanto pro Spark e pro MinIO foi feito utilizando Docker, o que permite a reproducibilidade do ambiente e minimiza erros de execuÃ§Ã£o da pipeline.